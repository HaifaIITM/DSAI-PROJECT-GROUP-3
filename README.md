# State-Space Echo State Networks for Multi-Horizon Financial Forecasting

> **Course:** DS & AI Lab
> **Repo status:** **Milestones 1â€“4 completed**
> â€“ M1: Problem Definition & Literature Review
> â€“ M2: Dataset Preparation, Features, Walk-Forward Splits
> â€“ M3: Model Architecture (ESN) + Deep Baselines (LSTM/Transformer/TCN) + Viz
> â€“ **M4: Training runs, hyper-parameter sweeps, fold/horizon aggregation, findings**

---

## ğŸ” Project Overview

We study **Echo State Networks (ESNs)** as **state-space models (SSMs)** for **multi-horizon financial forecasting** at daily frequency. ESNs maintain a latent â€œreservoirâ€ state (leaky, random recurrent) and train only a **ridge readout**, which controls variance under noisy, nonstationary returns. We benchmark against **LSTM**, **Transformer encoder**, and **Temporal ConvNet (TCN)** under a **leakage-safe walk-forward** protocol.

* **Tags:** DL Â· Time-Series Â· Finance Â· SSM Â· Reservoir Computing
* **Targets:** forward log-returns at **1, 5, 20** days
* **Validation:** rolling **10y train / 1y test / 1y step**; **train-only** normalization
* **Open data:** Yahoo Finance (primary); optional exogenous series planned.

---

## âœ… Whatâ€™s New in Milestone 4

* **Experiment config & grids:** `config/experiments.py` with small, reproducible grids for ESN/LSTM/Transformer/TCN.
* **Training runner:** `src/train/runner.py` to run **grid sweeps** across **folds Ã— horizons**, save artifacts, and auto-aggregate.
* **Unique experiment IDs:** filesystem-safe **param slugs** to separate runs; seeds fixed for reproducibility.
* **Aggregation utilities:** fold/horizon summaries in `src/eval/aggregate.py`; leaderboard helpers in notebook.
* **Findings auto-report:** notebook cell writes `data/experiments/milestone4_findings_<h>.txt`.

---

## ğŸ—‚ï¸ Repository Structure

```
esn-finance/
â”œâ”€ main.ipynb
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â”œâ”€ config/
â”‚  â”œâ”€ settings.py                 # tickers, dates, dirs, split sizes, features/targets
â”‚  â””â”€ experiments.py              # NEW: default grids, folds, horizons (M4)
â”œâ”€ src/
â”‚  â”œâ”€ pipeline.py                 # download â†’ process â†’ splits â†’ materialize â†’ run_baseline
â”‚  â”œâ”€ utils/
â”‚  â”‚  â””â”€ io.py
â”‚  â”œâ”€ data/
â”‚  â”‚  â”œâ”€ download.py              # yfinance downloaders
â”‚  â”‚  â”œâ”€ loader.py                # robust Yahoo CSV reader (two formats)
â”‚  â”‚  â””â”€ features.py              # feature engineering + targets
â”‚  â”œâ”€ splits/
â”‚  â”‚  â””â”€ walkforward.py           # split plan + leakage-safe scaling per fold
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ registry.py              # {"ridge","esn","lstm","transformer","tcn"} â†’ class
â”‚  â”‚  â”œâ”€ ridge_readout.py
â”‚  â”‚  â”œâ”€ esn.py                   # leaky reservoir + ridge readout
â”‚  â”‚  â”œâ”€ lstm.py                  # sequence-to-one, left-padded windows
â”‚  â”‚  â”œâ”€ transformer.py           # encoder + positional encoding
â”‚  â”‚  â””â”€ tcn.py                   # causal dilated ConvNet
â”‚  â”œâ”€ train/
â”‚  â”‚  â”œâ”€ __init__.py
â”‚  â”‚  â”œâ”€ utils.py                 # seeds, dict grid, param slug
â”‚  â”‚  â””â”€ runner.py                # run_experiment / run_sweep (M4 core)
â”‚  â”œâ”€ eval/
â”‚  â”‚  â”œâ”€ metrics.py               # RMSE/MAE/RÂ²/DirAcc + sign backtest (toy)
â”‚  â”‚  â””â”€ aggregate.py             # collect & summarize metrics across folds
â”‚  â””â”€ viz/
â”‚     â””â”€ plots.py                 # metrics table, bars, residuals, cum-PnL, time series
â””â”€ data/
   â”œâ”€ raw/                        # OHLCV CSVs (git-ignored)
   â”œâ”€ processed/                  # *_features.csv (PX, features, targets)
   â”œâ”€ splits/                     # splits.json + fold_k/{train.csv,test.csv,scaler.json}
   â””â”€ experiments/                # <model>/<exp_id>/fold_k/{preds_*.csv, metrics_*.json}
```

---

## âš™ï¸ Setup

```bash
# Recommended
conda create -n esn-finance python=3.11 -y
conda activate esn-finance
pip install -r requirements.txt
```

**requirements.txt (core):**

```
numpy
pandas
scikit-learn
matplotlib
yfinance
tqdm
torch
```

---

## ğŸš€ Quickstart (M2â€“M3)

Open **`main.ipynb`** and run:

1. **Download** raw data â†’ `data/raw/`
2. **Process** features/targets â†’ `data/processed/`
3. **Build** `splits.json` â†’ `data/splits/`
4. **Materialize** fold files with **train-only** scalers â†’ `data/splits/fold_k/`
5. **Train/Eval** quick baselines: `"ridge"`, `"esn"`, `"lstm"`, `"transformer"`, `"tcn"`

Visualization helpers live in `src/viz/plots.py`.

---

## ğŸ§ª Milestone 4: Run Sweeps & Report

Inside **`main.ipynb`**:

* Configure grids/folds/horizons via `config/experiments.py`.
* Use `run_sweep(model_name, grid, folds, horizons, exp_prefix)` from `src/train/runner.py`.
* Artifacts are saved under `data/experiments/<model>/<exp_id>/fold_<k>/`.
* Build **leaderboards** and **findings** automatically (cells included in notebook).

**Artifacts per fold:**

* `preds_<h>.csv` (y_true, y_pred indexed by date)
* `metrics_<h>.json` (RMSE/MAE/RÂ²/DirAcc + backtest stats)
* `summary_by_horizon.csv` (meanÂ±std across folds written per exp_id)

---

## ğŸ“¦ Data & Features

* **Symbols**: `^GSPC`, `SPY`, `BTC-USD`, `ETH-USD`, `^NSEI`, `^NSEBANK`, `RELIANCE.NS`, `TCS.NS`, `EURUSD=X`, `USDINR=X`, `GC=F`, `CL=F`, `^VIX`.
* **Features** (computed on Adj Close if present): `ret_1`, `ret_2`, `ret_5`, `vol_20`, `ma_10`, `ma_20`, `ma_gap`, `rsi_14`, `vol_z`, `dow`.
* **Targets**: `target_h1`, `target_h5`, `target_h20` (forward log-returns).
* **Leakage control**: walk-forward splits; **scaler fit on train only**; scaler params stored.

---

## ğŸ§  Models

* **Ridge**: strong linear floor on standardized features.
* **ESN**: leaky reservoir (`tanh`), spectral radius, washout, **ridge readout**.
* **LSTM**: sequence-to-one; left-padded windows for full alignment.
* **Transformer (encoder)**: positional encoding; last-token head.
* **TCN**: causal 1-D dilated ConvNet with residual blocks.

All expose a uniform API: `.fit(X, y)` / `.predict(X)` and are registered in `src/models/registry.py`.

---

## ğŸ“Š Snapshot: Initial Cross-Model Comparison (Fold 0, SPY, h=1)

| model       | fold | horizon   | rmse    | mae     | r2       | dir_acc   | avg_daily_pnl | vol     | sharpe    | hit_ratio | turnover |
| ----------- | ---- | --------- | ------- | ------- | -------- | --------- | ------------- | ------- | --------- | --------- | -------- |
| ridge       | 0    | target_h1 | 0.00834 | 0.00599 | -0.00520 | 0.492     | 0.000077      | 0.00832 | 0.147     | 0.492     | 0.687    |
| lstm        | 0    | target_h1 | 0.00939 | 0.00703 | -0.27641 | 0.508     | 0.000208      | 0.00833 | 0.396     | 0.508     | 0.337    |
| **esn**     | 0    | target_h1 | 0.01010 | 0.00785 | -0.47538 | 0.528     | **0.000841**  | 0.00828 | **1.612** | 0.528     | 0.853    |
| transformer | 0    | target_h1 | 0.01437 | 0.01109 | -1.98638 | 0.480     | -0.000127     | 0.00833 | -0.242    | 0.480     | 0.456    |
| tcn         | 0    | target_h1 | 0.02857 | 0.01933 | -10.8069 | **0.552** | 0.000433      | 0.00831 | 0.826     | 0.552     | 0.631    |

**Interpretation (early):**

* **Best magnitude (RMSE/MAE):** *Ridge* (linear floor is hard to beat on daily returns).
* **Best risk-adjusted trading signal:** *ESN* (highest Sharpe and avg PnL with decent DirAcc).
* **TCN** shows strong **directional accuracy** but poor calibration (high RMSE).
* **Transformer** underperforms with current small-data/short-training settings.

> Results are **single-fold** sanity checks; full evaluation will average across folds/horizons (M5).

---

## ğŸ” Reproducibility & Artifacts

* **Seeds** fixed in trainers; **scaler.json** per fold stores means/scales.
* **Param slugs** (hash of canonicalized params) name experiment folders to avoid overwrites.
* Every experiment is a **pure function** of (data split, hyper-params, seed).

---

## ğŸ§­ Roadmap

* **M1 (Oct 3)** âœ… Problem & Literature
* **M2 (Oct 10)** âœ… Data, features, walk-forward splits
* **M3 (Oct 17)** âœ… ESN + deep baselines + viz
* **M4 (Oct 31)** âœ… Training sweeps, aggregation, findings
* **M5 (Nov 7)** â–¶ Robust evaluation across folds/horizons; error analysis
* **M6 (Nov 14)** â–¶ Minimal demo/API (HF Spaces or Gradio), docs, final report

---

## ğŸ”’ Ethics & Responsible Use

This repository is for **education/research** only and **not** investment advice. Backtests are simplified (toy costs) and not indicative of live performance. We avoid look-ahead bias and data snooping; all scaling is train-only.

---

## ğŸ“š References (core)

* Jaeger, H. (2001); LukoÅ¡eviÄius & Jaeger (2009): Echo State Networks / Reservoir Computing
* Jaeger, H., et al. (2007): Leaky-Integrator ESNs
* Durbin, J., & Koopman, S. (2012): *Time Series Analysis by State Space Methods*
* Hyndman, R., & Athanasopoulos, G.: *Forecasting: Principles and Practice*
* Gneiting, T., & Raftery, A. (2007): Proper scoring rules
* Diebold, F., & Mariano, R. (1995): Comparing predictive accuracy
* Bailey, D., et al. (2014): Backtest overfitting

---

## ğŸ‘¥ Contributors

Team **GROUP-3** â€” feel free to open an Issue for questions, bugs, or feature requests.
